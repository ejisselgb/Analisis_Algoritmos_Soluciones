{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto Final Curso: Análisis de Algoritmos parte 2\n",
    "\n",
    "Contenido del documento:\n",
    "\n",
    "1. Estudio y comparación de los algoritmos\n",
    "2. Resultados del estudio\n",
    "3. Aplicabilidad y caso éxito\n",
    "4. Conclusiones\n",
    "5. Bibliografía"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Estudio de los algoritmos\n",
    "\n",
    "- Autocodificadores variacionales factorizados:\n",
    "\n",
    "Para comprender el comportamiento de los Autocodificadores variaciones factorizados propuestos en el trabajo \"Factorized Variational Autoencoders for Modeling Audience Reactions to Movies\" con el fin de identificar las reacciones de los usuarios al ver una serie de películas [ref], es necesario llevar a cabo el estudio de las técnicas que lo conforman: 1) factorización de tensores y 2) factorización de tensores no lineal.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 2.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 2.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 2.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 2.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n",
      "['Astaire,_Fred' 'Bacall,_Lauren' 'Bergman,_Ingrid_(I)' 'Bogart,_Humphrey'\n",
      " 'Brando,_Marlon' 'Cagney,_James' 'Colbert,_Claudette' 'Cooper,_Gary_(I)'\n",
      " 'Crawford,_Joan_(I)' 'Davis,_Bette' 'Dean,_James_(I)' 'Dietrich,_Marlene'\n",
      " 'Douglas,_Kirk_(I)' 'Fonda,_Henry' 'Fontaine,_Joan_(I)' 'Ford,_John_(I)'\n",
      " 'Gable,_Clark' 'Garbo,_Greta' 'Gardner,_Ava' 'Garland,_Judy'\n",
      " 'Gish,_Lillian' 'Grant,_Cary' 'Harlow,_Jean' 'Hawks,_Howard'\n",
      " 'Hayworth,_Rita' 'Hepburn,_Audrey' 'Hepburn,_Katharine'\n",
      " 'Heston,_Charlton' 'Hitchcock,_Alfred_(I)' 'Holden,_William_(I)'\n",
      " 'Keaton,_Buster' 'Kelly,_Gene_(I)' 'Kelly,_Grace_(I)' 'Lancaster,_Burt'\n",
      " 'Leigh,_Janet' 'Leigh,_Vivien' 'Loren,_Sophia' 'Marx,_Chico'\n",
      " 'Marx,_Groucho' 'Marx,_Harpo' 'Marx,_Zeppo' 'Monroe,_Marilyn'\n",
      " 'Olivier,_Laurence' 'Peck,_Gregory' 'Pickford,_Mary' 'Power,_Tyrone'\n",
      " 'Rogers,_Ginger' 'Stanwyck,_Barbara' 'Stewart,_James_(I)'\n",
      " 'Taylor,_Elizabeth_(I)' 'Temple,_Shirley' 'Tracy,_Spencer_(I)'\n",
      " 'Wayne,_John_(I)' 'Welles,_Orson' 'West,_Mae']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorly as tl\n",
    "\n",
    "PATH = 'C:/Users/pc/Desktop/Universidad/AnalisisAlgoritmos/Analisis_Algoritmos_Soluciones/ProyectoFinal/dataset/GoldenAgeOfHollywood_networkdata/'\n",
    "PATHNAME = 'HollywoodGoldenAge_names.txt'\n",
    "graphs = []\n",
    "\n",
    "for filename in os.listdir(PATH):\n",
    "    if filename.endswith(\"_s2.txt\"):\n",
    "        df = pd.read_csv(PATH+filename, header=None, sep='\\t')\n",
    "        tmp = df.values\n",
    "        np.fill_diagonal(tmp, 0)\n",
    "        graphs.append(tmp)\n",
    "\n",
    "graphs = np.asarray(graphs)\n",
    "graphs = np.swapaxes(graphs, 0, 2)\n",
    "graphs = graphs.astype(float)\n",
    "\n",
    "print(graphs)\n",
    "\n",
    "names = pd.read_csv(PATH+PATHNAME, header=None, sep=' ').values\n",
    "names = names[:,0]\n",
    "\n",
    "print(names)\n",
    "\n",
    "tensor = tl.sensor(graphs)\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
